{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangChain 필요성\n",
    "\n",
    "- OpenAI API 직접 사용시 \n",
    "    - 사용자 질문 -> LLM 호출 -> 응답\n",
    "\n",
    "- 실제 서비스에서 필요한 기능\n",
    "    - PDF 문서 안에서 답 찾기\n",
    "    - DB 조회 후 결과 설명\n",
    "    - 이전 대화 기억\n",
    "    - 여러 단계 추론\n",
    "\n",
    "#### LangChain 핵심 개념\n",
    "- Chain : 여러 단계를 연결한 처리 흐름\n",
    "    - ex) 검색 + 요약 + 생성 같은 다단계 흐름 구성 가능\n",
    "- Prompt Template : 프롬프트를 템플릿화\n",
    "- Memory : 이전 대화 저장\n",
    "- Retriever (검색기)\n",
    "- Agent : LLM 이 스스로 도구를 선택하여 실행\n",
    "\n",
    "#### 활용 가능 분야\n",
    "- 문서기반 QA\n",
    "    - ex) 사내문서 QA 챗봇, PDF 기반 질의응답, 법률 문서 검색, 기술 매뉴얼 검색\n",
    "- 챗봇 시스템\n",
    "- 데이터 분석 AI\n",
    "- 에이전트 기반 자동화 시스템\n",
    "\n",
    "### RAG(Retrieval Augmented Generation)\n",
    "- 검색 증강 생성\n",
    "- 특정 자료(문서, PDF, DB 등)에 대해 질문에 답할 수 있다.\n",
    "- 2가지 방식 제공\n",
    "    - RAG Agent 방식 : Agent 를 이용해 여러번 검색이 가능하고 복잡한 질문 처리 가능\n",
    "    - Two-step RAG Chain : 단순한 형태 / 한 번 호출\n",
    "\n",
    "- 처리 단계\n",
    "    - 1) Indexing : 데이터를 어떤 소스로부터 가져와서 검색할 수 있도록 정리(PDF->텍스트 추출->분할->임베딩->벡터DB 저장)\n",
    "    - 2) Retrieval and Generation : 실제 RAG가 동작하는 단계(사용자가 질문을 입력하면 인덱스에서 관련 데이터를 검색하고 그 데이터를 모델에 전달하여 답변을 생성)\n",
    "\n",
    "#### 정리\n",
    "- LLM 기반 기능을 체계적으로 사용할 수 있도록 제공되는 오픈소스 프레임워크\n",
    "- 외부 데이터 연결, 문서검색, 도구사용(API 호출), 멀티스텝 추론, 메모리 유지 대화 같은 복합적인 AI 애플리케이션 구조를 쉽게 만들 수 있도록 도와주는 도구\n",
    "\n",
    "<img src=\"https://xeblog.s3.ap-northeast-2.amazonaws.com/langchain_builder_9f58224358.jpg\" width=\"700\" height=\"604\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.13 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain_openai) (1.2.15)\n",
      "Requirement already satisfied: openai<3.0.0,>=2.20.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain_openai) (2.21.0)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain_openai)\n",
      "  Using cached tiktoken-0.12.0-cp314-cp314-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.7.6)\n",
      "Requirement already satisfied: packaging>=23.2.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (26.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.12.5)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (9.1.4)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (4.15.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.14.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.11.7)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.32.5)\n",
      "Requirement already satisfied: xxhash>=3.0.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.6.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (0.13.0)\n",
      "Requirement already satisfied: sniffio in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from openai<3.0.0,>=2.20.0->langchain_openai) (4.67.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.13->langchain_openai) (0.4.2)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain_openai)\n",
      "  Downloading regex-2026.2.19-cp314-cp314-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.13->langchain_openai) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=2.20.0->langchain_openai) (0.4.6)\n",
      "Downloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
      "Using cached tiktoken-0.12.0-cp314-cp314-win_amd64.whl (921 kB)\n",
      "Downloading regex-2026.2.19-cp314-cp314-win_amd64.whl (280 kB)\n",
      "Installing collected packages: regex, tiktoken, langchain_openai\n",
      "\n",
      "   ------------- -------------------------- 1/3 [tiktoken]\n",
      "   -------------------------- ------------- 2/3 [langchain_openai]\n",
      "   -------------------------- ------------- 2/3 [langchain_openai]\n",
      "   ---------------------------------------- 3/3 [langchain_openai]\n",
      "\n",
      "Successfully installed langchain_openai-1.1.10 regex-2026.2.19 tiktoken-0.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import bs4\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\source\\pythonsource\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# 랭체인에서 제공하는 로더 \n",
    "from langchain_community.document_loaders import YoutubeLoader, WebBaseLoader, PyPDFLoader \n",
    "\n",
    "# 임베딩 처리\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 문서 내용 임베딩\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# 모델 생성\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# 메세지\n",
    "from langchain.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# 프롬프트 작성\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "\n",
    "# 글을 쪼갤 때 사용하는 라이브러리\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Tools\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# 크롤링\n",
    "from langchain_community.document_loaders.firecrawl import FireCrawlLoader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "# 방법 1. \n",
    "# model = init_chat_model(model=\"gpt-4.1-mini\", temperature=0, max_tokes=500, timeout=60)\n",
    "\n",
    "# 방법 2\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Langchain\n",
    "- 모델 호출\n",
    "    - 1) invoke() : 모델이 답변을 전체 생성 후 응답\n",
    "    - 2) stream() : 모델이 생성한 답변을 중간중간 응답(논문요약, 보고서 생성, 코드 생성, 긴 설명)\n",
    "    - 3) batch() : 여러 개 요청을 한 번에 처리(병렬 처리 가능 - 리뷰 100 개 요약, 문단 50개 번역..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='사과는 건강에 매우 유익한 과일로 다양한 효능을 가지고 있습니다. 주요 효능은 다음과 같습니다:\\n\\n1. **풍부한 영양소**  \\n   사과에는 비타민 C, 비타민 A, 칼륨, 식이섬유 등이 풍부하게 들어 있어 전반적인 건강 유지에 도움을 줍니다.\\n\\n2. **소화 건강 개선**  \\n   사과에 포함된 식이섬유, 특히 펙틴 성분은 장내 유익균의 먹이가 되어 장 건강을 증진시키고 변비 예방에 효과적입니다.\\n\\n3. **체중 관리**  \\n   저칼로리에 포만감을 주는 과일로 식사 전 간식으로 섭취하면 과식을 방지하고 체중 조절에 도움이 됩니다.\\n\\n4. **심장 건강 증진**  \\n   사과 속의 항산화 물질과 식이섬유는 혈중 콜레스테롤 수치를 낮추고 혈압을 조절하는 데 도움을 줘 심혈관 질환 예방에 도움이 됩니다.\\n\\n5. **항산화 작용**  \\n   사과에는 플라보노이드와 폴리페놀 같은 항산화제가 풍부해 세포 손상을 막고 노화 방지에 기여합니다.\\n\\n6. **혈당 조절**  \\n   식이섬유가 혈당 상승을 완만하게 해 당뇨병 관리에도 긍정적인 영향을 미칩니다.\\n\\n7. **면역력 강화**  \\n   비타민 C와 같은 영양소가 면역 체계를 강화하여 감염병 예방에 도움이 됩니다.\\n\\n이처럼 사과는 간편하게 섭취할 수 있으면서도 여러 면에서 건강에 긍정적인 효과를 주는 과일입니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 391, 'prompt_tokens': 13, 'total_tokens': 404, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a391f2cee0', 'id': 'chatcmpl-DD0PPOhBd9uYnI0UaRPr173uJQu3r', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c92f7-4e0a-7870-a350-53ce2184ed79-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 13, 'output_tokens': 391, 'total_tokens': 404, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "{'input_tokens': 13, 'output_tokens': 391, 'total_tokens': 404, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "사과는 건강에 매우 유익한 과일로 다양한 효능을 가지고 있습니다. 주요 효능은 다음과 같습니다:\n",
      "\n",
      "1. **풍부한 영양소**  \n",
      "   사과에는 비타민 C, 비타민 A, 칼륨, 식이섬유 등이 풍부하게 들어 있어 전반적인 건강 유지에 도움을 줍니다.\n",
      "\n",
      "2. **소화 건강 개선**  \n",
      "   사과에 포함된 식이섬유, 특히 펙틴 성분은 장내 유익균의 먹이가 되어 장 건강을 증진시키고 변비 예방에 효과적입니다.\n",
      "\n",
      "3. **체중 관리**  \n",
      "   저칼로리에 포만감을 주는 과일로 식사 전 간식으로 섭취하면 과식을 방지하고 체중 조절에 도움이 됩니다.\n",
      "\n",
      "4. **심장 건강 증진**  \n",
      "   사과 속의 항산화 물질과 식이섬유는 혈중 콜레스테롤 수치를 낮추고 혈압을 조절하는 데 도움을 줘 심혈관 질환 예방에 도움이 됩니다.\n",
      "\n",
      "5. **항산화 작용**  \n",
      "   사과에는 플라보노이드와 폴리페놀 같은 항산화제가 풍부해 세포 손상을 막고 노화 방지에 기여합니다.\n",
      "\n",
      "6. **혈당 조절**  \n",
      "   식이섬유가 혈당 상승을 완만하게 해 당뇨병 관리에도 긍정적인 영향을 미칩니다.\n",
      "\n",
      "7. **면역력 강화**  \n",
      "   비타민 C와 같은 영양소가 면역 체계를 강화하여 감염병 예방에 도움이 됩니다.\n",
      "\n",
      "이처럼 사과는 간편하게 섭취할 수 있으면서도 여러 면에서 건강에 긍정적인 효과를 주는 과일입니다.\n"
     ]
    }
   ],
   "source": [
    "# 모델 호출 : invoke()\n",
    "# 하나의 메시지 입력 시 \n",
    "\n",
    "res = model.invoke(\"사과의 효능 설명\")\n",
    "print(res)\n",
    "print(res.usage_metadata)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "사과의 효능 설명\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "사과는 맛도 좋고 건강에도 여러 가지 이로운 점이 많은 과일입니다. 사과의 주요 효능은 다음과 같습니다:\n",
      "\n",
      "1. **항산화 작용**  \n",
      "   사과에는 폴리페놀, 비타민 C 등 항산화 물질이 풍부해 체내 활성산소를 억제하고 노화 방지에 도움을 줍니다.\n",
      "\n",
      "2. **소화 개선**  \n",
      "   식이섬유가 풍부해 장 운동을 활성화시키고 변비 예방에 효과적입니다.\n",
      "\n",
      "3. **심혈관 건강 증진**  \n",
      "   사과에 들어있는 펙틴과 플라보노이드가 콜레스테롤 수치를 낮춰 주어 심장병과 고혈압 위험을 줄여줍니다.\n",
      "\n",
      "4. **체중 관리 도움**  \n",
      "   저칼로리에 식이섬유가 많아 포만감을 오래 유지시켜 다이어트에 도움이 됩니다.\n",
      "\n",
      "5. **혈당 조절**  \n",
      "   식이섬유가 혈당 상승을 완만하게 해서 당뇨 관리를 돕는 효과가 있습니다.\n",
      "\n",
      "6. **면역력 강화**  \n",
      "   비타민 C와 각종 미네랄이 면역 기능을 향상시키고 감염 예방에 도움을 줍니다.\n",
      "\n",
      "7. **구강 건강**  \n",
      "   사과는 씹는 과정에서 침 분비를 촉진해 구강 내 세균 증식을 억제하고 치아 건강에 도움을 줍니다.\n",
      "\n",
      "이처럼 사과는 그냥 먹어도 맛있고, 다양한 건강상의 이점을 가진 과일이므로 꾸준히 섭취하는 것이 좋습니다.\n"
     ]
    }
   ],
   "source": [
    "# stream() : 모델이 생성한 답변을 중간중간 응답\n",
    "\n",
    "agent = create_agent(model)\n",
    "\n",
    "# stream_mode = \"values\" : 현재 상태 전체\n",
    "# stream_mode = \"updates\" : update 내용만\n",
    "for step in agent.stream({\"messages\":[{\"role\":\"user\",\"content\":\"사과의 효능 설명\"}]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과는 건강에 유익한 다양한 성분을 포함하고 있어 여러 가지 효능이 있습니다. 대표적인 사과의 효능은 다음과 같습니다:\n",
      "\n",
      "1. **소화 개선**  \n",
      "   사과에는 식이섬유가 풍부하게 들어 있어 장 운동을 촉진하고 변비 예방에 도움을 줍니다.\n",
      "\n",
      "2. **심혈관 건강 증진**  \n",
      "   사과에 들어있는 폴리페놀과 항산화 물질은 혈관을 건강하게 유지하고 콜레스테롤 수치를 낮추어 심장 질환 예방에 효과적입니다.\n",
      "\n",
      "3. **면역력 강화**  \n",
      "   비타민 C와 항산화제가 포함되어 있어 면역 체계를 강화하고 감염에 대한 저항력을 높여줍니다.\n",
      "\n",
      "4. **체중 관리**  \n",
      "   낮은 칼로리와 풍부한 식이섬유 덕분에 포만감을 주어 다이어트에 도움이 됩니다.\n",
      "\n",
      "5. **뇌 건강 도움**  \n",
      "   사과에 포함된 항산화 물질은 뇌 세포 손상을 줄이고 인지 기능 향상에 긍정적인 영향을 미친다는 연구 결과가 있습니다.\n",
      "\n",
      "6. **혈당 조절**  \n",
      "   사과의 수용성 식이섬유는 혈당 상승을 완만하게 해주어 당뇨 환자에게도 도움이 될 수 있습니다.\n",
      "\n",
      "이 외에도 사과는 피부 건강 개선이나 항염 작용 등 다양한 건강상의 이점을 가지고 있으니 꾸준히 섭취하는 것이 좋습니다.\n",
      "바나나는 맛도 좋고 여러 가지 건강에 좋은 효능을 가지고 있는 과일입니다. 주요 효능을 설명해드리면 다음과 같습니다:\n",
      "\n",
      "1. **에너지 공급**  \n",
      "   바나나는 탄수화물이 풍부해 빠른 에너지 공급원으로 좋습니다. 특히 운동 전후에 섭취하면 에너지 보충에 도움이 됩니다.\n",
      "\n",
      "2. **소화 개선**  \n",
      "   바나나에는 식이섬유가 포함되어 있어 장 운동을 촉진하고 변비 예방에 효과적입니다.\n",
      "\n",
      "3. **심장 건강**  \n",
      "   바나나는 칼륨이 많이 들어 있어 혈압을 조절하고 심장 건강을 지키는 데 도움을 줍니다.\n",
      "\n",
      "4. **기분 개선과 스트레스 감소**  \n",
      "   바나나에 들어있는 트립토판 성분은 세로토닌으로 전환되어 기분을 좋게 하고 불안과 스트레스를 완화하는 데 도움을 줄 수 있습니다.\n",
      "\n",
      "5. **면역력 강화**  \n",
      "   비타민 C와 여러 미네랄을 함유하여 면역 체계를 강화하는 역할을 합니다.\n",
      "\n",
      "6. **체중 관리**  \n",
      "   포만감을 주는 식이섬유 덕분에 다이어트 시에도 간식으로 적합합니다.\n",
      "\n",
      "이 외에도 바나나는 혈당 조절, 눈 건강, 뼈 건강 등에 긍정적인 영향을 줄 수 있습니다. 단, 너무 많이 섭취하면 칼로리가 쌓일 수 있으니 적당량을 유지하는 것이 좋습니다.\n",
      "오렌지는 맛도 좋고 건강에도 매우 유익한 과일입니다. 오렌지의 주요 효능은 다음과 같습니다:\n",
      "\n",
      "1. **비타민 C 공급**  \n",
      "   오렌지는 비타민 C가 풍부하여 면역력을 강화하고 감기 예방에 도움을 줍니다. 또한 피부 건강에도 좋으며 항산화 작용으로 노화 방지에 효과적입니다.\n",
      "\n",
      "2. **항산화 작용**  \n",
      "   플라보노이드, 베타카로틴 등 다양한 항산화 물질이 포함되어 있어 세포 손상을 방지하고 만성질환 위험을 낮춥니다.\n",
      "\n",
      "3. **심혈관 건강 개선**  \n",
      "   오렌지에 들어있는 섬유질과 칼륨은 혈압 조절과 혈관 건강 유지에 도움을 주어 심장병 예방에 기여합니다.\n",
      "\n",
      "4. **소화 촉진**  \n",
      "   식이섬유가 풍부해 소화를 돕고 변비 예방에 효과적입니다.\n",
      "\n",
      "5. **체내 염증 완화**  \n",
      "   항염증 성분이 있어 염증 완화와 관련된 질환에 도움을 줄 수 있습니다.\n",
      "\n",
      "6. **체중 관리**  \n",
      "   칼로리가 낮고 포만감을 주기 때문에 다이어트 중 간식으로 적합합니다.\n",
      "\n",
      "이처럼 오렌지는 맛뿐만 아니라 건강 전반에 긍정적인 영향을 미치는 과일입니다.\n"
     ]
    }
   ],
   "source": [
    "# batch() \n",
    "inputs = [\n",
    "    \"사과의 효능을 설명해줘\",\n",
    "    \"바나나의 효능을 설명해줘\",\n",
    "    \"오렌지의 효능을 설명해줘\",\n",
    "]\n",
    "\n",
    "res = model.batch(inputs)\n",
    "\n",
    "for r in res :\n",
    "    print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Messages\n",
    "    - https://docs.langchain.com/oss/python/langchain/messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "system_msg = SystemMessage(\"You are a helpful assistant.\")\n",
    "human_msg = HumanMessage(\"Hello, how are you?\")\n",
    "\n",
    "res = model.invoke([system_msg, human_msg])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Korea was considered the most controversial team in the 2002 FIFA World Cup primarily due to several contentious refereeing decisions during their matches in the knockout stages, which some fans, players, and analysts viewed as heavily favoring them.\n",
      "\n",
      "Key reasons for the controversy include:\n",
      "\n",
      "1. **Refereeing Decisions**: In both the Round of 16 match against Italy and the quarter-final against Spain, there were multiple questionable calls. For example:\n",
      "   - Against Italy, a golden goal by Ahn Jung-hwan was initially ruled offside but then allowed to stand.\n",
      "   - Several fouls and possible penalties against South Korea were not called.\n",
      "   - In the Spain match, Spain had two clear goals disallowed, and some Italian and Spanish fans felt that the referees showed bias.\n",
      "\n",
      "2. **Home Advantage and Pressure**: South Korea was co-hosting the tournament with Japan, and some argued that the home advantage influenced referees or added pressure on officials to favor the host nation.\n",
      "\n",
      "3. **Unexpected Run**: South Korea reached the semi-finals for the first time in history, defeating traditional football powers Italy and Spain en route, which fueled speculation about whether their success was due to merit or controversial officiating.\n",
      "\n",
      "Overall, while South Korea played well and were celebrated for their historic achievement, the refereeing controversies made their 2002 World Cup campaign one of the most debated in football history.\n"
     ]
    }
   ],
   "source": [
    "# 여러 개의 메시지 리스트\n",
    "# https://docs.langchain.com/oss/python/langchain/models#invoke\n",
    "# 대화기록\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"Which country was the most controversial in 2002 World Cup?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"assistant\",\n",
    "            \"content\":\"South Korea\"\n",
    "        },\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\":\"Explain why that country was so controversial.\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "res = model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tools : 앞에서 했었던 function calling \n",
    "    - https://docs.langchain.com/oss/python/langchain/tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "위,경도 37.566, 126.9784\n",
      "[HumanMessage(content='Seoul 날씨 어때?', additional_kwargs={}, response_metadata={}, id='4a532bcd-bd93-4756-9f67-c65bb0975344'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 70, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a391f2cee0', 'id': 'chatcmpl-DD2BTYgCTSB3H3ypS1WK06jf1Y8jw', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c935f-6dda-7d51-8a2f-431e4b4b29cc-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Seoul', 'timezone': 'Asia/Seoul'}, 'id': 'call_uXq6HUIsmkKz1s9Mvn8jQEQH', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 70, 'output_tokens': 22, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='{\"location\": \"Seoul\", \"latitute\": 37.566, \"longitude\": 126.9784, \"temperature_c\": 12.1, \"windspeed_kmh\": 3.2, \"winddirection_deg\": 63, \"weathercode\": 0, \"time\": \"2026-02-25T14:45\"}', name='get_weather', id='084b73bc-6fe9-4635-88c8-7fde41c00755', tool_call_id='call_uXq6HUIsmkKz1s9Mvn8jQEQH'), AIMessage(content='서울의 현재 날씨는 맑고 기온은 약 12.1도입니다. 바람은 시속 3.2km로 동북동 방향으로 불고 있습니다. 더 궁금한 점 있으시면 말씀해 주세요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 176, 'total_tokens': 228, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_a391f2cee0', 'id': 'chatcmpl-DD2BXoWGobrlSMMT1lyQxv8Lev6gY', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c935f-8099-7443-8893-84a710a29766-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 176, 'output_tokens': 52, 'total_tokens': 228, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "서울의 현재 날씨는 맑고 기온은 약 12.1도입니다. 바람은 시속 3.2km로 동북동 방향으로 불고 있습니다. 더 궁금한 점 있으시면 말씀해 주세요!\n"
     ]
    }
   ],
   "source": [
    "# 앞에서 했었던 function calling \n",
    "import json\n",
    "import requests\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "@tool\n",
    "def get_weather(location, timezone):\n",
    "    \"\"\"지역명을 이용해 위,경도를 찾은 후, 위,경도로 해당 날씨 가져오기\"\"\"\n",
    "\n",
    "    # 위,경도 찾기\n",
    "    params = {\"name\":location,\"count\":1,\"language\":\"ko\"}\n",
    "    geo = requests.get(\"https://geocoding-api.open-meteo.com/v1/search\",params=params,timeout=30).json()\n",
    "    \n",
    "    results = geo.get(\"results\")[0]\n",
    "\n",
    "    if not results:\n",
    "        return json.dumps({\"error\": f\"{location} 위치를 찾지 못했습니다\"})\n",
    "\n",
    "    lat, lon = results.get('latitude'), results.get('longitude')\n",
    "    print(f\"위,경도 {lat}, {lon}\")\n",
    "\n",
    "\n",
    "    # 날씨 api 호출\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    weather_params = {\n",
    "        \"latitude\":lat,\n",
    "        \"longitude\":lon,\n",
    "        \"current_weather\":\"true\",\n",
    "        \"timezone\":timezone\n",
    "    }\n",
    "    weather_result = requests.get(url,params=weather_params,timeout=30).json()\n",
    "\n",
    "    current_weather = weather_result.get('current_weather')\n",
    "\n",
    "    if not current_weather:\n",
    "        return json.dumps({\"error\": \"날씨 데이터를 가져오지 못했습니다\"})\n",
    "\n",
    "    # 받은 결과를 모델에게 돌려주기 위한 json 구조 설정\n",
    "    return json.dumps({\n",
    "        \"location\":location,\n",
    "        \"latitute\":lat,\n",
    "        \"longitude\":lon,\n",
    "        \"temperature_c\": current_weather.get(\"temperature\"),\n",
    "        \"windspeed_kmh\":current_weather.get(\"windspeed\"),       \n",
    "        \"winddirection_deg\":current_weather.get(\"winddirection\"),\n",
    "        \"weathercode\":current_weather.get(\"weathercode\"),\n",
    "        \"time\":current_weather.get(\"time\"),\n",
    "    })\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "agent = create_agent(model=model, tools=tools)\n",
    "res = agent.invoke({\"messages\":[{\"role\":\"user\",\"content\":\"Seoul 날씨 어때?\"}]})\n",
    "print(res['messages'])\n",
    "\n",
    "res['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 을 사용한 RAG\n",
    "#### [실습 1] blog 를 기반으로 질의 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "query\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_9UjzcUaqIZIIQ8EObbLOdYci)\n",
      " Call ID: call_9UjzcUaqIZIIQ8EObbLOdYci\n",
      "  Args:\n",
      "    query: query\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "('Source : {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: pytest\\ndataclasses\\n\\nSource : {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', [Document(id='2b1e6760-2a82-4a8e-9c0c-1439188e8c3b', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='pytest\\ndataclasses'), Document(id='5143ffca-32f3-4ba6-a8a9-16ef20b4075f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",')])\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You queried the term \"query.\" The retrieved content from the source https://lilianweng.github.io/posts/2023-06-23-agent/ includes mentions of \"pytest,\" \"dataclasses,\" and some conversational samples. If you have a more specific question or need details related to a particular topic, please let me know!\n"
     ]
    }
   ],
   "source": [
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    header_template={\n",
    "        \"User-Agent\": \"my-langchain-rag-app\"\n",
    "    },\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# print(docs)\n",
    "# 양이 큰 문서 -> 원하는 크기로 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# 전체 문서를 text_splitter에서 적용한대로 잘라줘\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "## 검색을 어떻게 해올 것인가?\n",
    "# 1단계: embedding 모델 정하기/생성\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# 2단계: 문서의 벡터화 / 벡터 DB 저장\n",
    "vector_store = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "\n",
    "@tool # function_calling 에 사용\n",
    "def retrieve_context(query):\n",
    "    \"\"\"사용자 질문을 받아 벡터 검색 수행 / 관련 문서 2개 가져온 후 텍스트 형태로 반환\"\"\"\n",
    "\n",
    "    ## (이전:openai) 질문-사용자의 답변 벡터화\n",
    "    ## (langchian) similarity_search로 query에 대한 임베딩이 자동으로 생성\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join((f\"Source : {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "                for doc in retrieved_docs)\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "# tools = [ 함수명 ]으로 사용\n",
    "tools = [retrieve_context]\n",
    "\n",
    "# prompt 정의\n",
    "prompt = \"\"\"\n",
    "You have access to a tool that retrieves context from a blog post.\n",
    "Use the tool to help answer user queries\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(model, tools, system_prompt=prompt)\n",
    "\n",
    "query = \"What is task decomposition?\"\n",
    "\n",
    "for step in agent.stream({\"messages\":[{\"role\":\"user\", \"content\":\"query\"}]}, stream_mode=\"values\"):\n",
    "    step['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [실습 2] PDF 기반 질의탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 문서 로드드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 텍스트 분할\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 벡터 스토어 생성\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 검색 tool 작성\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 질문 던지기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [실습 3] 유튜브 영상 자막 추출 후 내용요약\n",
    "\n",
    "- YoutubeLoader 를 이용한 자막 추출\n",
    "- RecursiveCharacterTextSplitter 를 이용한 일정한 크기로 자막 분할\n",
    "- agent 를 이용한 LLM 호출 \n",
    "- 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 유튜브 자막 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 문서 분할\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3. Map 단계 (각 chunk 요약)\n",
    "# -----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4. Reduce 단계 (전체 결합 요약)\n",
    "# -----------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firecrawl\n",
    "- 웹사이트를 크롤링하여 LLM(Long Library Management)에서 사용할 수 있는 데이터로 변환\n",
    "- 접근 가능한 모든 하위 페이지를 크롤링하고 각 페이지에 대한 깔끔한 마크다운과 메타데이터를 제공\n",
    "- https://docs.langchain.com/oss/python/integrations/document_loaders/firecrawl (api 키 발급 필요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: firecrawl-py in c:\\source\\pythonsource\\.venv\\lib\\site-packages (4.16.2)\n",
      "Requirement already satisfied: requests in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (2.32.5)\n",
      "Requirement already satisfied: httpx in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (1.2.1)\n",
      "Requirement already satisfied: websockets in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (16.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (2.12.5)\n",
      "Requirement already satisfied: aiohttp in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from firecrawl-py) (3.13.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic>=2.0->firecrawl-py) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic>=2.0->firecrawl-py) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic>=2.0->firecrawl-py) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from pydantic>=2.0->firecrawl-py) (0.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from aiohttp->firecrawl-py) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->firecrawl-py) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx->firecrawl-py) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx->firecrawl-py) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpx->firecrawl-py) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->firecrawl-py) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from requests->firecrawl-py) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\source\\pythonsource\\.venv\\lib\\site-packages (from requests->firecrawl-py) (2.6.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install firecrawl-py\n",
    "\n",
    "# from firecrawl import Firecrawl\n",
    "\n",
    "# app = Firecrawl(api_key=\"fc-912e77013d054309a06641ad7a6df948\")\n",
    "\n",
    "# # Scrape a website:\n",
    "# app.scrape('firecrawl.dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# mode=\"scrape\" : 지정한 url 페이지 내용 가져오기\n",
    "# WebBaseLoader()\n",
    "\n",
    "# mode=\"crawl\" : 하위 페이지까지 내용 크롤링\n",
    "loader = FireCrawlLoader(url=\"https://firecrawl.dev\", mode=\"scrape\")\n",
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "\n",
    "loader = FireCrawlLoader(url=\"https://naver.com\", mode=\"crawl\", params={\"limit\":5})\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [실습] react 페이지 크롤링 후 크롤링 문서 기반으로 RAG 적용\n",
    "- 1. Ingest(색인) 단계: URL들 → Firecrawl → Document → Split → Embedding → FAISS 저장\n",
    "- 2. Query(질의) 단계: 질문 → FAISS 검색(top-k) → (질문 + 근거) → LLM 답변"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "loader = FireCrawlLoader(\n",
    "    url=\"https://ko.react.dev/reference/react/\",\n",
    "    mode=\"crawl\",\n",
    "    params={\n",
    "        \"includePaths\": [r\"^/reference/react/use.*\"], \n",
    "        \"limit\": 100,  # 넓게 크롤링\n",
    "    }\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "def get_url(doc):\n",
    "    md = doc.metadata or {}\n",
    "    return md.get(\"source_url\") or md.get(\"source\") or md.get(\"url\") or \"(unknown)\"\n",
    "\n",
    "\n",
    "# use~ 시작하는 url 만 걸러내기\n",
    "pattern = re.compile(r\"^https://ko\\.react\\.dev/reference/react/use.*\")\n",
    "use_docs = [d for d in docs if pattern.match(get_url(d))] \n",
    "print(len(use_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is useCallBack?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  retrieve_context (call_FS6J1DhwEB6OyflXC1vjfPwj)\n",
      " Call ID: call_FS6J1DhwEB6OyflXC1vjfPwj\n",
      "  Args:\n",
      "    query: useCallback\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: retrieve_context\n",
      "\n",
      "Source : https://ko.react.dev/reference/react/useCallback\n",
      "Content: #### 주의 사항 [Link for 주의 사항 ](https://ko.react.dev/reference/react/useCallback\\#caveats \"Link for 주의 사항 \")\n",
      "\n",
      "- `useCallback`은 Hook이므로, **컴포넌트의 최상위 레벨** 또는 커스텀 Hook에서만 호출할 수 있습니다. 반복문이나 조건문 내에서 호출할 수 없습니다. 이 작업이 필요하다면 새로운 컴포넌트로 분리해서 state를 새 컴포넌트로 옮기세요.\n",
      "- React는 **특별한 이유가 없는 한 캐시 된 함수를 삭제하지 않습니다.** 예를 들어 개발 환경에서는 컴포넌트 파일을 편집할 때 React가 캐시를 삭제합니다. 개발 환경과 프로덕션 환경 모두에서, 초기 마운트 중에 컴포넌트가 일시 중단되면 React는 캐시를 삭제합니다. 앞으로 React는 캐시 삭제를 활용하는 더 많은 기능을 추가할 수 있습니다. 예를 들어, React에 가상화된 목록에 대한 빌트인 지원이 추가한다면, 가상화된 테이블 뷰포트에서 스크롤 밖의 항목에 대해 캐시를 삭제하는것이 적절할 것 입니다. 이는 `useCallback`을 성능 최적화 방법으로 의존하는 경우에 개발자의 예상과 일치해야 합니다. 그렇지 않다면 [state 변수](https://ko.react.dev/reference/react/useState#im-trying-to-set-state-to-a-function-but-it-gets-called-instead) 나 [ref](https://ko.react.dev/reference/react/useRef#avoiding-recreating-the-ref-contents) 가 더 적절할 수 있습니다.\n",
      "\n",
      "* * *\n",
      "\n",
      "## 용법 [Link for 용법 ](https://ko.react.dev/reference/react/useCallback\\#usage \"Link for 용법 \")\n",
      "\n",
      "Source : https://ko.react.dev/reference/react/useCallback\n",
      "Content: - [컴포넌트가 렌더링 될 때마다 `useCallback`이 다른 함수를 반환합니다.](https://ko.react.dev/reference/react/useCallback#every-time-my-component-renders-usecallback-returns-a-different-function)\n",
      "  - [반복문에서 각 항목마다 `useCallback`을 호출하고 싶지만, 이는 허용되지 않습니다.](https://ko.react.dev/reference/react/useCallback#i-need-to-call-usememo-for-each-list-item-in-a-loop-but-its-not-allowed)\n",
      "\n",
      "Source : https://ko.react.dev/reference/react/useCallback\n",
      "Content: - [레퍼런스](https://ko.react.dev/reference/react/useCallback#reference)\n",
      "  - [`useCallback(fn, dependencies)`](https://ko.react.dev/reference/react/useCallback#usecallback)\n",
      "- [용법](https://ko.react.dev/reference/react/useCallback#usage)\n",
      "  - [컴포넌트의 리렌더링 건너뛰기](https://ko.react.dev/reference/react/useCallback#skipping-re-rendering-of-components)\n",
      "  - [Memoized 콜백에서 상태 업데이트하기](https://ko.react.dev/reference/react/useCallback#updating-state-from-a-memoized-callback)\n",
      "  - [Effect가 너무 자주 실행되는 것을 방지하기](https://ko.react.dev/reference/react/useCallback#preventing-an-effect-from-firing-too-often)\n",
      "  - [커스텀 Hook 최적화하기](https://ko.react.dev/reference/react/useCallback#optimizing-a-custom-hook)\n",
      "- [문제 해결](https://ko.react.dev/reference/react/useCallback#troubleshooting)\n",
      "  - [컴포넌트가 렌더링 될 때마다 `useCallback`이 다른 함수를 반환합니다.](https://ko.react.dev/reference/react/useCallback#every-time-my-component-renders-usecallback-returns-a-different-function)\n",
      "\n",
      "Source : https://ko.react.dev/reference/react/useCallback\n",
      "Content: * * *\n",
      "\n",
      "## 레퍼런스 [Link for 레퍼런스 ](https://ko.react.dev/reference/react/useCallback\\#reference \"Link for 레퍼런스 \")\n",
      "\n",
      "### `useCallback(fn, dependencies)` [Link for this heading](https://ko.react.dev/reference/react/useCallback\\#usecallback \"Link for this heading\")\n",
      "\n",
      "리렌더링 간에 함수 정의를 캐싱하려면 컴포넌트의 최상단에서 `useCallback`을 호출하세요.\n",
      "\n",
      "```\n",
      "import { useCallback } from 'react';\n",
      "\n",
      "export default function ProductPage({ productId, referrer, theme }) {\n",
      "\n",
      "  const handleSubmit = useCallback((orderDetails) => {\n",
      "\n",
      "    post('/product/' + productId + '/buy', {\n",
      "\n",
      "      referrer,\n",
      "\n",
      "      orderDetails,\n",
      "\n",
      "    });\n",
      "\n",
      "  }, [productId, referrer]);\n",
      "```\n",
      "\n",
      "[아래에서 더 많은 예시를 확인해보세요.](https://ko.react.dev/reference/react/useCallback#usage)\n",
      "\n",
      "#### 매개변수 [Link for 매개변수 ](https://ko.react.dev/reference/react/useCallback\\#parameters \"Link for 매개변수 \")\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "`useCallback`은 React Hook 중 하나로, 함수 컴포넌트에서 함수의 메모이제이션(캐싱)을 도와주는 역할을 합니다.\n",
      "\n",
      "- 주요 역할: 컴포넌트가 리렌더링 될 때마다 동일한 함수 인스턴스를 재사용하도록 하여, 불필요한 함수 재생성을 방지함\n",
      "- 사용처:\n",
      "  - 함수가 자식 컴포넌트에 props로 전달되어 자식 컴포넌트가 불필요하게 재렌더링 되는 것을 막고 싶을 때\n",
      "  - 함수 의존성 배열에 포함된 값이 변경될 때만 새로운 함수를 생성\n",
      "  - 성능 최적화를 위해 콜백 함수를 메모이제이션 할 때\n",
      "- 사용법: `useCallback(fn, dependencies)` 형태, 첫 번째 인자로 함수, 두 번째 인자는 의존성 배열\n",
      "- 제한사항: Hook이므로 컴포넌트 최상위나 커스텀 Hook에서만 호출 가능, 반복문이나 조건문 안에서는 호출 불가\n",
      "\n",
      "예제 코드:\n",
      "```javascript\n",
      "import { useCallback } from 'react';\n",
      "\n",
      "function Component({ productId, referrer }) {\n",
      "  const handleSubmit = useCallback((orderDetails) => {\n",
      "    post('/product/' + productId + '/buy', {\n",
      "      referrer,\n",
      "      orderDetails,\n",
      "    });\n",
      "  }, [productId, referrer]);\n",
      "}\n",
      "```\n",
      "\n",
      "- React는 특별한 이유가 없으면 캐시된 함수를 삭제하지 않으며, 개발 및 프로덕션 환경에서 일부 조건 시 캐시를 삭제할 수도 있음\n",
      "- 무분별한 사용 시 오히려 성능 저하를 일으킬 수 있으므로 필요한 경우에만 사용 권장\n",
      "\n",
      "출처:  \n",
      "https://ko.react.dev/reference/react/useCallback\n"
     ]
    }
   ],
   "source": [
    "# 벡터 작업\n",
    "# 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# 전체 문서 : text_splitter 적용 -> 분할\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vector_store = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query):\n",
    "    \"\"\"사용자 질문을 받아 벡터 검색 수행 / 관련 문서 2개 가져온 후 텍스트 형태로 반환\"\"\"\n",
    "\n",
    "    ## query에 대한 임배딩 자동 생성\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=4)\n",
    "    serialized = \"\\n\\n\".join((f\"Source : {get_url(doc)}\\nContent: {doc.page_content}\")\n",
    "                for doc in retrieved_docs)\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools = [retrieve_context]\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "너는 React 공식 문서(한국어)를 기반으로 답변하는 도우미이ㅑ.\n",
    "공식 문서 안에서만 대답해. 모르면 모른다고 말해. \n",
    "답변은 한국어로, 핵심은 불릿으로 정리하고, 마지막에 출처 url 나열해. \n",
    "\"\"\"\n",
    "agent = create_agent(model, tools, system_prompt=prompt)\n",
    "\n",
    "query=\"What is useCallBack?\"\n",
    "\n",
    "for step in agent.stream({\"messages\":[{\"role\":\"user\",\"content\":query}]}, stream_mode=\"values\"):\n",
    "    step[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
